{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'books_textAnalytics': (537, 5),\n",
       " 'SimpleLiving': (957, 5),\n",
       " 'Liberal': (999, 5),\n",
       " 'Parenting': (972, 5),\n",
       " 'mentalhealth_textAnalytics': (989, 5),\n",
       " 'Atheism': (963, 5),\n",
       " 'Feminism': (990, 5),\n",
       " 'OutOfTheLoop_textAnalytics': (986, 5),\n",
       " 'relationships_textAnalytics': (894, 5),\n",
       " 'PoliticalDebate_textAnalytics': (996, 5),\n",
       " 'NeutralPolitics': (982, 5),\n",
       " 'changemyview_textAnalaytics': (964, 5)}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the path where all the files are stored\n",
    "data_folder = \"E:/tcd/Text analytics/Paper/text_analytics_data\"\n",
    "\n",
    "# List of all the file names provided by the user\n",
    "file_names = [\n",
    "    \"books_textAnalytics.xlsx\", \"SimpleLiving.xlsx\", \"Liberal.xlsx\", \"Parenting.xlsx\",\n",
    "    \"mentalhealth_textAnalytics.xlsx\", \"Atheism.xlsx\", \"Feminism.xlsx\", \"OutOfTheLoop_textAnalytics.xlsx\",\n",
    "    \"relationships_textAnalytics.xlsx\", \"PoliticalDebate_textAnalytics.xlsx\",\n",
    "    \"NeutralPolitics.xlsx\", \"changemyview_textAnalaytics.xlsx\"\n",
    "]\n",
    "\n",
    "# Load all data files into a dictionary of DataFrames\n",
    "dataframes = {}\n",
    "for file in file_names:\n",
    "    file_path = os.path.join(data_folder, file)\n",
    "    df = pd.read_excel(file_path)\n",
    "    dataframes[file.replace(\".xlsx\", \"\")] = df\n",
    "\n",
    "# Display basic information about the loaded datasets\n",
    "summary_info = {name: df.shape for name, df in dataframes.items()}\n",
    "summary_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: \n",
    "üîç What is Tentative Language?\n",
    "Tentative language includes words or phrases that express uncertainty, possibility, or hesitation. Instead of stating something with full confidence, people using tentative language show that they are not entirely sure or are being careful about making bold claims.\n",
    "\n",
    "üß† Examples of tentative words/phrases:\n",
    "\n",
    "maybe\n",
    "\n",
    "I think\n",
    "\n",
    "possibly\n",
    "\n",
    "seems\n",
    "\n",
    "could\n",
    "\n",
    "I suppose\n",
    "\n",
    "not sure\n",
    "\n",
    "These words soften a statement and often appear when someone is being polite, careful, open to discussion, or expressing a personal opinion.\n",
    "\n",
    "üìå Why Did We Analyze Tentative Language?\n",
    "In our research on Reddit communities, we're studying how language reflects group identity, inclusion, exclusion, and emotional tone.\n",
    "\n",
    "Tentative language is important because it can:\n",
    "\n",
    "Show how open or cautious users are in different communities\n",
    "\n",
    "Help us compare support-oriented vs. ideological spaces\n",
    "\n",
    "Reveal how people position themselves in conversations ‚Äî either confidently or uncertainly\n",
    "\n",
    "üß™ How We Did It (The Method):\n",
    "We created a list of common tentative phrases (e.g., maybe, perhaps, I think).\n",
    "\n",
    "We scanned all Reddit posts and comments in each subreddit dataset.\n",
    "\n",
    "For every comment, we counted:\n",
    "\n",
    "How many tentative words it contained.\n",
    "\n",
    "How many total words it had.\n",
    "\n",
    "We then calculated the average number of tentative words per 1,000 words of text, so we could fairly compare subreddits of different lengths.\n",
    "\n",
    "üìä What This Tells Us:\n",
    "By comparing these averages, we can see which communities tend to be more cautious, empathetic, or uncertain in their language ‚Äî and which ones are more direct, assertive, or confident.\n",
    "\n",
    "For example, a mental health subreddit might have higher tentative language (e.g., \"I think this might help you\") ‚Äî showing care, empathy, and non-judgment.\n",
    "\n",
    "An ideological or debate subreddit might use less tentative and more assertive language ‚Äî signaling confidence or group alignment (e.g., \"This is clearly wrong\", \"We must act\").\n",
    "\n",
    "This helps us understand how people communicate differently based on the type of community they‚Äôre in.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Subreddit  Avg_Tentative_per_1000  \\\n",
      "0             books_textAnalytics                5.467418   \n",
      "1                    SimpleLiving                4.693294   \n",
      "2                         Liberal                6.414107   \n",
      "3                       Parenting                4.961209   \n",
      "4      mentalhealth_textAnalytics                6.577861   \n",
      "5                         Atheism                5.386393   \n",
      "6                        Feminism                4.888109   \n",
      "7      OutOfTheLoop_textAnalytics                7.366483   \n",
      "8     relationships_textAnalytics                5.152606   \n",
      "9   PoliticalDebate_textAnalytics                5.770503   \n",
      "10                NeutralPolitics                3.680489   \n",
      "11    changemyview_textAnalaytics                6.508076   \n",
      "\n",
      "    Avg_Assertive_per_1000  \n",
      "0                 3.487895  \n",
      "1                 3.369054  \n",
      "2                 3.167521  \n",
      "3                 4.504882  \n",
      "4                 5.309452  \n",
      "5                 4.143656  \n",
      "6                 3.491745  \n",
      "7                 3.389369  \n",
      "8                 5.480767  \n",
      "9                 2.475240  \n",
      "10                1.095138  \n",
      "11                2.848714  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "# Tentative and Assertive word lists\n",
    "tentative_words = [\n",
    "    \"maybe\", \"perhaps\", \"possibly\", \"might\", \"could\", \"I think\", \"I guess\", \"not sure\",\n",
    "    \"I feel like\", \"seems\", \"appears\", \"probably\", \"likely\", \"I suppose\"\n",
    "]\n",
    "assertive_words = [\n",
    "    \"definitely\", \"certainly\", \"clearly\", \"obviously\", \"must\", \"always\", \"never\", \"undoubtedly\",\n",
    "    \"without a doubt\", \"no question\", \"it's clear\", \"I know\", \"in fact\"\n",
    "]\n",
    "\n",
    "# Compile regex patterns for speed\n",
    "tentative_pattern = re.compile(r'\\b(' + '|'.join(map(re.escape, tentative_words)) + r')\\b', re.IGNORECASE)\n",
    "assertive_pattern = re.compile(r'\\b(' + '|'.join(map(re.escape, assertive_words)) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "# Function to count tentative/assertive words\n",
    "def count_tentative_assertive(text):\n",
    "    if not isinstance(text, str):\n",
    "        return 0, 0\n",
    "    tentative_count = len(tentative_pattern.findall(text))\n",
    "    assertive_count = len(assertive_pattern.findall(text))\n",
    "    return tentative_count, assertive_count\n",
    "\n",
    "# Analyze all files\n",
    "results = []\n",
    "for file in file_names:\n",
    "    path = os.path.join(data_folder, file)\n",
    "    df = pd.read_excel(path)\n",
    "\n",
    "    # Use the 'body' column for text content\n",
    "    df['tentative_count'], df['assertive_count'] = zip(*df['body'].fillna(\"\").map(count_tentative_assertive))\n",
    "    total_words = df['body'].fillna(\"\").str.split().map(len)\n",
    "\n",
    "    # Normalize counts per 1,000 words\n",
    "    df['tentative_per_1000'] = df['tentative_count'] / total_words * 1000\n",
    "    df['assertive_per_1000'] = df['assertive_count'] / total_words * 1000\n",
    "\n",
    "    # Store average metrics\n",
    "    results.append({\n",
    "        'Subreddit': file.replace(\".xlsx\", \"\"),\n",
    "        'Avg_Tentative_per_1000': df['tentative_per_1000'].mean(),\n",
    "        'Avg_Assertive_per_1000': df['assertive_per_1000'].mean()\n",
    "    })\n",
    "\n",
    "# Create result DataFrame\n",
    "summary_df = pd.DataFrame(results)\n",
    "\n",
    "# Display results\n",
    "print(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Summary CSV saved as 'linguistic_feature_tentative_language_summary.csv'\n"
     ]
    }
   ],
   "source": [
    "summary_df.to_csv(\"linguistic_feature_tentative_language_summary.csv\", index=False)\n",
    "print(\"‚úÖ Summary CSV saved as 'linguistic_feature_tentative_language_summary.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß† Cognitive vs. Emotional Language Analysis\n",
    "This method looks at whether communities use more:\n",
    "\n",
    "üß† Cognitive words ‚Äî like think, know, understand, reason\n",
    "\n",
    "‚ù§Ô∏è Emotional words ‚Äî like happy, sad, angry, love\n",
    "\n",
    "It helps show whether a community leans more toward rational discussion or emotional expression ‚Äî a key difference between ideological, support, and hobby-based communities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Subreddit  Avg_Emotional_per_1000  \\\n",
      "0             books_textAnalytics                2.788764   \n",
      "1                    SimpleLiving                5.166001   \n",
      "2                         Liberal                1.643860   \n",
      "3                       Parenting                3.783355   \n",
      "4      mentalhealth_textAnalytics                5.562809   \n",
      "5                         Atheism                2.642290   \n",
      "6                        Feminism                2.386102   \n",
      "7      OutOfTheLoop_textAnalytics                1.425974   \n",
      "8     relationships_textAnalytics                4.812065   \n",
      "9   PoliticalDebate_textAnalytics                0.776728   \n",
      "10                NeutralPolitics                0.304071   \n",
      "11    changemyview_textAnalaytics                1.316108   \n",
      "\n",
      "    Avg_Cognitive_per_1000  \n",
      "0                 8.208269  \n",
      "1                 6.804397  \n",
      "2                 9.560669  \n",
      "3                 8.075761  \n",
      "4                12.662897  \n",
      "5                14.474337  \n",
      "6                11.577813  \n",
      "7                10.897819  \n",
      "8                11.383405  \n",
      "9                10.052581  \n",
      "10                3.466521  \n",
      "11               10.796612  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define emotional and cognitive word lists (you can expand these if needed)\n",
    "emotional_words = [\n",
    "    \"happy\", \"sad\", \"angry\", \"love\", \"hate\", \"joy\", \"fear\", \"excited\", \"depressed\",\n",
    "    \"worried\", \"upset\", \"proud\", \"frustrated\", \"disappointed\", \"grateful\"\n",
    "]\n",
    "\n",
    "cognitive_words = [\n",
    "    \"think\", \"know\", \"understand\", \"realize\", \"believe\", \"because\", \"reason\",\n",
    "    \"consider\", \"assume\", \"logic\", \"idea\", \"explain\", \"analyze\", \"decide\", \"conclude\"\n",
    "]\n",
    "\n",
    "# Compile regex patterns for efficiency\n",
    "emotional_pattern = re.compile(r'\\b(' + '|'.join(map(re.escape, emotional_words)) + r')\\b', re.IGNORECASE)\n",
    "cognitive_pattern = re.compile(r'\\b(' + '|'.join(map(re.escape, cognitive_words)) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "# Function to count emotional and cognitive words\n",
    "def count_emotional_cognitive(text):\n",
    "    if not isinstance(text, str):\n",
    "        return 0, 0\n",
    "    emotional_count = len(emotional_pattern.findall(text))\n",
    "    cognitive_count = len(cognitive_pattern.findall(text))\n",
    "    return emotional_count, cognitive_count\n",
    "\n",
    "# Run analysis and collect results\n",
    "results = []\n",
    "for file in file_names:\n",
    "    path = os.path.join(data_folder, file)\n",
    "    df = pd.read_excel(path)\n",
    "    \n",
    "    # Process the 'body' column containing text\n",
    "    df['emotional_count'], df['cognitive_count'] = zip(*df['body'].fillna(\"\").map(count_emotional_cognitive))\n",
    "    total_words = df['body'].fillna(\"\").str.split().map(len)\n",
    "\n",
    "    df['emotional_per_1000'] = df['emotional_count'] / total_words * 1000\n",
    "    df['cognitive_per_1000'] = df['cognitive_count'] / total_words * 1000\n",
    "\n",
    "    results.append({\n",
    "        'Subreddit': file.replace(\".xlsx\", \"\"),\n",
    "        'Avg_Emotional_per_1000': df['emotional_per_1000'].mean(),\n",
    "        'Avg_Cognitive_per_1000': df['cognitive_per_1000'].mean()\n",
    "    })\n",
    "\n",
    "# Display final summary\n",
    "summary_df = pd.DataFrame(results)\n",
    "print(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Summary CSV saved as 'linguistic_feature_cognitive_emotional_language_summary.csv'\n"
     ]
    }
   ],
   "source": [
    "summary_df.to_csv(\"linguistic_feature_cognitive_emotional_language_summary.csv\", index=False)\n",
    "print(\"‚úÖ Summary CSV saved as 'linguistic_feature_cognitive_emotional_language_summary.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Politeness & Rudeness Detection\n",
    "What it measures:\n",
    "Counts how often polite or rude words appear in comments/posts, normalized per 1,000 words.\n",
    "\n",
    "Why it matters:\n",
    "Shows how respectful, empathetic, or confrontational a community is in its language.\n",
    "\n",
    "Examples of words used:\n",
    "\n",
    "Polite: please, thank you, sorry, appreciate\n",
    "\n",
    "Rude: idiot, dumb, shut up, nonsense\n",
    "\n",
    "What it tells us:\n",
    "Communities like r/relationships or r/mentalhealth may use more polite expressions. Ideological or debate-focused subreddits may show more harsh or blunt expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Question Usage\n",
    "What it measures:\n",
    "Counts how many question marks ? appear per 1,000 words of text.\n",
    "\n",
    "Why it matters:\n",
    "Shows how inquisitive or conversational a subreddit is. High question frequency often means people are curious, seeking info, or trying to engage discussion.\n",
    "\n",
    "What it tells us:\n",
    "Support and discussion-based subreddits (like r/OutOfTheLoop or r/Parenting) may contain more questions. Debate subreddits might have rhetorical or challenge-style questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Lexical Diversity (Vocabulary Richness)\n",
    "What it measures:\n",
    "The variety of words used ‚Äî calculated as the number of unique words / total words (called the type-token ratio).\n",
    "\n",
    "Why it matters:\n",
    "It shows how varied and rich the language is. High diversity often means thoughtful, original responses. Low diversity can reflect repetitiveness or echo chambers.\n",
    "\n",
    "What it tells us:\n",
    "Communities like r/books or r/SimpleLiving may have richer vocabularies. Others may reuse common language or memes more often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Subreddit  Avg_Polite_per_1000  Avg_Rude_per_1000  \\\n",
      "0             books_textAnalytics             4.917442           0.320266   \n",
      "1                    SimpleLiving             2.954352           0.438402   \n",
      "2                         Liberal             3.542858           0.814871   \n",
      "3                       Parenting             2.458847           0.329102   \n",
      "4      mentalhealth_textAnalytics             3.008317           1.681955   \n",
      "5                         Atheism             2.148128           1.390569   \n",
      "6                        Feminism             3.578229           0.537317   \n",
      "7      OutOfTheLoop_textAnalytics             2.568467           0.716645   \n",
      "8     relationships_textAnalytics             1.894586           0.425835   \n",
      "9   PoliticalDebate_textAnalytics             1.244648           0.446062   \n",
      "10                NeutralPolitics             1.320073           0.123185   \n",
      "11    changemyview_textAnalaytics             1.166852           0.609144   \n",
      "\n",
      "    Avg_Questions_per_1000  Avg_Lexical_Diversity  \n",
      "0                 8.403188               0.535229  \n",
      "1                13.325005               0.698822  \n",
      "2                27.405796               0.515510  \n",
      "3                11.930927               0.686958  \n",
      "4                 7.173995               0.657734  \n",
      "5                13.803189               0.560321  \n",
      "6                18.655088               0.455198  \n",
      "7                48.390448               0.852542  \n",
      "8                 5.250144               0.522387  \n",
      "9                11.121637               0.668929  \n",
      "10               19.115016               0.703519  \n",
      "11                3.733715               0.621533  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Word lists\n",
    "polite_words = [\"please\", \"thank you\", \"thanks\", \"sorry\", \"appreciate\", \"kind\", \"welcome\", \"respect\", \"helpful\"]\n",
    "rude_words = [\"stupid\", \"idiot\", \"dumb\", \"shut up\", \"nonsense\", \"ignorant\", \"moron\", \"hate\", \"useless\"]\n",
    "\n",
    "# Compile regex for speed\n",
    "polite_pattern = re.compile(r'\\b(' + '|'.join(map(re.escape, polite_words)) + r')\\b', re.IGNORECASE)\n",
    "rude_pattern = re.compile(r'\\b(' + '|'.join(map(re.escape, rude_words)) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "# Count polite and rude words\n",
    "def count_polite_rude(text):\n",
    "    if not isinstance(text, str):\n",
    "        return 0, 0\n",
    "    polite = len(polite_pattern.findall(text))\n",
    "    rude = len(rude_pattern.findall(text))\n",
    "    return polite, rude\n",
    "\n",
    "# Count question marks\n",
    "def count_questions(text):\n",
    "    if not isinstance(text, str):\n",
    "        return 0\n",
    "    return text.count('?')\n",
    "\n",
    "# Calculate lexical diversity (type-token ratio)\n",
    "def lexical_diversity(text):\n",
    "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
    "        return 0\n",
    "    words = text.lower().split()\n",
    "    return len(set(words)) / len(words) if words else 0\n",
    "\n",
    "# Run analysis on each file\n",
    "results = []\n",
    "for file in file_names:\n",
    "    path = os.path.join(data_folder, file)\n",
    "    df = pd.read_excel(path)\n",
    "    df['body'] = df['body'].fillna(\"\")\n",
    "\n",
    "    # Apply all 3 functions\n",
    "    df['polite_count'], df['rude_count'] = zip(*df['body'].map(count_polite_rude))\n",
    "    df['question_count'] = df['body'].map(count_questions)\n",
    "    df['lexical_diversity'] = df['body'].map(lexical_diversity)\n",
    "    \n",
    "    # Normalize per 1000 words\n",
    "    total_words = df['body'].str.split().map(len)\n",
    "    df['polite_per_1000'] = df['polite_count'] / total_words * 1000\n",
    "    df['rude_per_1000'] = df['rude_count'] / total_words * 1000\n",
    "    df['questions_per_1000'] = df['question_count'] / total_words * 1000\n",
    "\n",
    "    results.append({\n",
    "        'Subreddit': file.replace(\".xlsx\", \"\"),\n",
    "        'Avg_Polite_per_1000': df['polite_per_1000'].mean(),\n",
    "        'Avg_Rude_per_1000': df['rude_per_1000'].mean(),\n",
    "        'Avg_Questions_per_1000': df['questions_per_1000'].mean(),\n",
    "        'Avg_Lexical_Diversity': df['lexical_diversity'].mean()\n",
    "    })\n",
    "\n",
    "# Create final summary DataFrame\n",
    "summary_df = pd.DataFrame(results)\n",
    "print(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Summary CSV saved as 'linguistic_feature_summary.csv'\n"
     ]
    }
   ],
   "source": [
    "# Save summary_df to CSV\n",
    "summary_df.to_csv(\"linguistic_feature_summary.csv\", index=False)\n",
    "print(\"‚úÖ Summary CSV saved as 'linguistic_feature_summary.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
